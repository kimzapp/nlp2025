{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b821d44",
   "metadata": {},
   "source": [
    "## A. Pytorch introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdab82b",
   "metadata": {},
   "source": [
    "### 1. Khám phá tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760fa4e",
   "metadata": {},
   "source": [
    "##### 1.1. Tạo tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62260490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor từ list:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Tensor từ NumPy array:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "\n",
      "Ones Tensor:\n",
      " tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "Random Tensor:\n",
      " tensor([[0.2970, 0.7112],\n",
      "        [0.0399, 0.6098]])\n",
      "\n",
      "Shape của tensor: torch.Size([2, 2])\n",
      "Datatype của tensor: torch.float32\n",
      "Device lưu trữ tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Tạo tensor từ list\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor từ list:\\n {x_data}\\n\")\n",
    "\n",
    "# Tạo tensor từ NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor từ NumPy array:\\n {x_np}\\n\")\n",
    "\n",
    "# Tạo tensor với các giá trị ngẫu nhiên hoặc hằng số\n",
    "x_ones = torch.ones_like(x_data) # tạo tensor gồm các số 1 có cùng shape với x_data\n",
    "print(f\"Ones Tensor:\\n {x_ones}\\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # tạo tensor ngẫu nhiên\n",
    "print(f\"Random Tensor:\\n {x_rand}\\n\")\n",
    "\n",
    "# In ra shape, dtype, và device của tensor\n",
    "print(f\"Shape của tensor: {x_rand.shape}\")\n",
    "print(f\"Datatype của tensor: {x_rand.dtype}\")\n",
    "print(f\"Device lưu trữ tensor: {x_rand.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fc776",
   "metadata": {},
   "source": [
    "##### 1.2. Các phép toán trên tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc0f380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kết quả phép cộng:\n",
      " tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "\n",
      "Kết quả phép nhân với scalar:\n",
      " tensor([[ 5, 10],\n",
      "        [15, 20]])\n",
      "\n",
      "Kết quả phép nhân ma trận:\n",
      " tensor([[ 5, 11],\n",
      "        [11, 25]])\n"
     ]
    }
   ],
   "source": [
    "# cộng x_data với chính nó\n",
    "y_data = x_data + x_data\n",
    "print(f\"\\nKết quả phép cộng:\\n {y_data}\")\n",
    "\n",
    "# nhân x_data với 5\n",
    "z_data = x_data * 5 \n",
    "print(f\"\\nKết quả phép nhân với scalar:\\n {z_data}\")\n",
    "\n",
    "# nhân x_data với chuyển vị của nó\n",
    "t_data = x_data @ x_data.T\n",
    "print(f\"\\nKết quả phép nhân ma trận:\\n {t_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f189b0",
   "metadata": {},
   "source": [
    "##### 1.3. Indexing và slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3bdcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hàng đầu của x_data:\n",
      " tensor([1, 2])\n",
      "\n",
      "Cột thứ hai của x_data:\n",
      " tensor([2, 4])\n",
      "\n",
      "Phần tử ở hàng thứ 2, cột thứ 2 của x_data:\n",
      " 4\n"
     ]
    }
   ],
   "source": [
    "# lấy ra hàng đầu của x_data\n",
    "row_0 = x_data[0, :]\n",
    "print(f\"\\nHàng đầu của x_data:\\n {row_0}\")\n",
    "\n",
    "# lấy ra cột thứ hai của x_data\n",
    "col_1 = x_data[:, 1]\n",
    "print(f\"\\nCột thứ hai của x_data:\\n {col_1}\")\n",
    "\n",
    "# lấy ra giá trị ở hàng thứ 2, cột thứ 2\n",
    "elem_1_1 = x_data[1, 1]\n",
    "print(f\"\\nPhần tử ở hàng thứ 2, cột thứ 2 của x_data:\\n {elem_1_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45fdd7",
   "metadata": {},
   "source": [
    "##### 1.4. Thay đổi hình dạng tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7b94bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor ban đầu có shape torch.Size([4, 4]):\n",
      " tensor([[0.0576, 0.1546, 0.6425, 0.0079],\n",
      "        [0.9428, 0.8543, 0.4260, 0.1635],\n",
      "        [0.3995, 0.2605, 0.3760, 0.2219],\n",
      "        [0.0585, 0.2314, 0.8772, 0.1416]])\n",
      "\n",
      "Tensor sau khi thay đổi shape thành torch.Size([16, 1]):\n",
      " tensor([[0.0576],\n",
      "        [0.1546],\n",
      "        [0.6425],\n",
      "        [0.0079],\n",
      "        [0.9428],\n",
      "        [0.8543],\n",
      "        [0.4260],\n",
      "        [0.1635],\n",
      "        [0.3995],\n",
      "        [0.2605],\n",
      "        [0.3760],\n",
      "        [0.2219],\n",
      "        [0.0585],\n",
      "        [0.2314],\n",
      "        [0.8772],\n",
      "        [0.1416]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.rand(4, 4)\n",
    "reshaped_tensor = random_tensor.view(16, 1)\n",
    "print(f\"\\nTensor ban đầu có shape {random_tensor.shape}:\\n {random_tensor}\")\n",
    "print(f\"\\nTensor sau khi thay đổi shape thành {reshaped_tensor.shape}:\\n {reshaped_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34010d24",
   "metadata": {},
   "source": [
    "### 2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34096b",
   "metadata": {},
   "source": [
    "##### 2.1. Thực hành với autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b9794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1.], requires_grad=True)\n",
      "y: tensor([3.], grad_fn=<AddBackward0>)\n",
      "grad_fn của y: <AddBackward0 object at 0x000002D443EB69E0>\n",
      "Đạo hàm của z theo x: tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "# Tạo một tensor và yêu cầu tính đạo hàm cho nó\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "# Thực hiện một phép toán\n",
    "y = x + 2\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "# y được tạo ra từ một phép toán có x, nên nó cũng có grad_fn\n",
    "print(f\"grad_fn của y: {y.grad_fn}\")\n",
    "\n",
    "# Thực hiện thêm các phép toán\n",
    "z = y * y * 3\n",
    "\n",
    "# Tính đạo hàm của z theo x\n",
    "z.backward() # tương đương z.backward(torch.tensor(1.))\n",
    "\n",
    "# Đạo hàm được lưu trong thuộc tính .grad\n",
    "# Ta có z = 3 * (x+2)^2 => dz/dx = 6 * (x+2). Với x=1, dz/dx = 18\n",
    "print(f\"Đạo hàm của z theo x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d19075",
   "metadata": {},
   "source": [
    "**? Câu hỏi**: Chuyện gì xảy ra nếu gọi z.backward() một lần nữa? Tại sao?\n",
    "\n",
    "**Trả lời**: Khi thực hiện các phép toán trên tensor, pytorch tạo một đồ thị tính toán để autograd có thể truy ngược lại và thực hiện backprop. Sau khi thực hiện `backward()` xong, graph đã được giải phóng, do đó, nếu gọi `backward()` thêm một lần nữa, ta sẽ gặp lỗi:\n",
    "\n",
    "`RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ea28d",
   "metadata": {},
   "source": [
    "### 3. Xây dựng mô hình đầu tiên với `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30398683",
   "metadata": {},
   "source": [
    "##### 3.1. Lớp `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f13e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 5])\n",
      "Output shape: torch.Size([3, 2])\n",
      "Output:\n",
      " tensor([[ 1.5633,  0.0229],\n",
      "        [ 0.7108, -1.0280],\n",
      "        [ 0.7114, -0.0930]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo một lớp Linear biến đổi từ 5 chiều -> 2 chiều\n",
    "linear_layer = torch.nn.Linear(in_features=5, out_features=2, bias=True)\n",
    "# Tạo một tensor đầu vào mẫu\n",
    "input_tensor = torch.randn(3, 5) # 3 mẫu, mỗi mẫu 5 chiều\n",
    "# Truyền đầu vào qua lớp linear\n",
    "output = linear_layer(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb2501",
   "metadata": {},
   "source": [
    "##### 3.2. Lớp `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32178398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4])\n",
      "Output shape: torch.Size([4, 3])\n",
      "Embeddings:\n",
      " tensor([[-0.6549, -0.6637,  0.3571],\n",
      "        [-0.5784,  0.8276, -1.6882],\n",
      "        [ 0.9680,  0.4330, -0.1997],\n",
      "        [ 0.0266,  1.3040, -0.1350]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo lớp Embedding cho một từ điển 10 từ, mỗi từ biểu diễn bằng vector 3 chiều\n",
    "embedding_layer = torch.nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "# Tạo một tensor đầu vào chứa các chỉ số của từ (ví dụ: một câu)\n",
    "# Các chỉ số phải nhỏ hơn 10\n",
    "input_indices = torch.LongTensor([1, 5, 0, 8])\n",
    "# Lấy ra các vector embedding tương ứng\n",
    "embeddings = embedding_layer(input_indices)\n",
    "print(f\"Input shape: {input_indices.shape}\")\n",
    "print(f\"Output shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings:\\n {embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725fdba",
   "metadata": {},
   "source": [
    "##### 3.3. Kết hợp thành một `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86015f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyFirstModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(MyFirstModel, self).__init__()\n",
    "        # Định nghĩa các lớp (layer) bạn sẽ dùng\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU() # Hàm kích hoạt\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, indices):\n",
    "        # Định nghĩa luồng dữ liệu đi qua các lớp\n",
    "        # 1. Lấy embedding\n",
    "        embeds = self.embedding(indices)\n",
    "        # 2. Truyền qua lớp linear và hàm kích hoạt\n",
    "        hidden = self.activation(self.linear(embeds))\n",
    "        # 3. Truyền qua lớp output\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "    \n",
    "# Khởi tạo và kiểm tra mô hình\n",
    "model = MyFirstModel(vocab_size=100, embedding_dim=16, hidden_dim=8, output_dim=2)\n",
    "input_data = torch.LongTensor([[1, 2, 5, 9]]) # một câu gồm 4 từ\n",
    "output_data = model(input_data)\n",
    "print(f\"Model output shape: {output_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6901d",
   "metadata": {},
   "source": [
    "## B. Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71637a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SimpleRNNForTokenClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):\n",
    "        super(SimpleRNNForTokenClassification, self).__init__()\n",
    "        # 1. Lớp Embedding: Chuyển đổi ID của từ thành vector dày đặc (dense vector).\n",
    "        # Ví dụ: từ có ID là 5 -> vector [0.1, 0.9, ..., 0.4]\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 2. Lớp RNN: Xử lý chuỗi vector và tạo ra hidden state tại mỗi bước.\n",
    "        # batch_first=True nghĩa là input và output sẽ có chiều batch ở đầu tiên.\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # 3. Lớp Linear: Ánh xạ từ hidden state (kích thước hidden_dim)\n",
    "        # sang không gian số lượng nhãn (kích thước num_tags).\n",
    "        self.linear = nn.Linear(hidden_dim, num_tags)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        # `sentence` là một chuỗi các ID của từ, có dạng (batch_size, seq_len).\n",
    "        # 1. Truyền câu qua lớp Embedding.\n",
    "        # Output `embeds` có dạng (batch_size, seq_len, embedding_dim).\n",
    "        embeds = self.embedding(sentence)\n",
    "        # 2. Truyền chuỗi embedding qua lớp RNN.\n",
    "        # `rnn_out` là output của RNN tại mỗi bước thời gian.\n",
    "        # `rnn_out` có dạng (batch_size, seq_len, hidden_dim).\n",
    "        rnn_out, _ = self.rnn(embeds)\n",
    "        # 3. Truyền output của RNN qua lớp Linear để lấy điểm số cho mỗi nhãn.\n",
    "        # `tag_scores` có dạng (batch_size, seq_len, num_tags).\n",
    "        tag_scores = self.linear(rnn_out)\n",
    "        # (Trong thực tế, chúng ta sẽ áp dụng Softmax lên tag_scores để có phân phối xác suất)\n",
    "        return tag_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
